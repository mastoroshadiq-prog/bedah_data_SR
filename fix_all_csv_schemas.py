"""
COMPREHENSIVE FIX: Align ALL CSV files with SQL schema
This fixes column mismatches between CSV files and Supabase tables
"""

import pandas as pd
import os

print("=" * 100)
print("COMPREHENSIVE CSV FIX - Aligning with SQL Schema")
print("=" * 100)

# ============================================================================
# FIX 1: blocks_standardized.csv
# ============================================================================
print("\n1. Fixing blocks_standardized.csv...")

df_blocks = pd.read_csv('output/normalized_tables/phase1_core/blocks_standardized.csv')

print(f"   Current columns: {list(df_blocks.columns)}")

# SQL schema expects: id, block_code, estate_code, estate_name, division, category, has_production_data, created_at
# CSV has: id, block_code, block_code_standardized, estate_code, estate_name, division, has_production_data, category

# Drop extra column: block_code_standardized
if 'block_code_standardized' in df_blocks.columns:
    df_blocks = df_blocks.drop('block_code_standardized', axis=1)
    print("   ✅ Dropped 'block_code_standardized'")

# Reorder to match SQL schema (without created_at - will be auto-generated)
column_order = ['id', 'block_code', 'estate_code', 'estate_name', 'division', 'category', 'has_production_data']
df_blocks = df_blocks[column_order]

# Save
df_blocks.to_csv('output/normalized_tables/phase1_core/blocks_standardized.csv', index=False)
print(f"   ✅ Saved with {len(df_blocks.columns)} columns: {list(df_blocks.columns)}")
print(f"   Records: {len(df_blocks)}")

# ============================================================================
# VERIFY ALL OTHER CSV FILES
# ============================================================================
print("\n" + "=" * 100)
print("Verifying other CSV files...")
print("=" * 100)

csv_files = {
    'estates': {
        'file': 'output/normalized_tables/phase1_core/estates.csv',
        'expected_cols': ['id', 'estate_code', 'estate_name']
    },
    'block_land_infrastructure': {
        'file': 'output/normalized_tables/phase2_metadata/block_land_infrastructure.csv',
        'drop_cols': []  # Will check dynamically
    },
    'block_pest_disease': {
        'file': 'output/normalized_tables/phase2_metadata/block_pest_disease.csv',
        'drop_cols': []
    },
    'block_planting_history': {
        'file': 'output/normalized_tables/phase2_metadata/block_planting_history.csv',
        'drop_cols': []
    },
    'block_planting_yearly': {
        'file': 'output/normalized_tables/phase2_metadata/block_planting_yearly.csv',
        'drop_cols': []
    },
    'production_annual': {
        'file': 'output/normalized_tables/phase3_production/production_annual.csv',
        'drop_cols': []
    },
    'production_monthly': {
        'file': 'output/normalized_tables/phase3_production/production_monthly.csv',
        'drop_cols': []
    }
}

for table_name, config in csv_files.items():
    file_path = config['file']
    
    if not os.path.exists(file_path):
        print(f"\n⚠️  {table_name}: File not found - {file_path}")
        continue
    
    df = pd.read_csv(file_path)
    print(f"\n{table_name}:")
    print(f"  Columns ({len(df.columns)}): {list(df.columns)[:5]}{'...' if len(df.columns) > 5 else ''}")
    print(f"  Records: {len(df)}")
    
    # Remove 'created_at' if exists (will be auto-generated by database)
    if 'created_at' in df.columns:
        df = df.drop('created_at', axis=1)
        df.to_csv(file_path, index=False)
        print(f"  ✅ Removed 'created_at' column")

print("\n" + "=" * 100)
print("✅ CSV ALIGNMENT COMPLETE!")
print("=" * 100)
print("\nNext steps:")
print("1. Make sure Supabase tables match the SQL schema")
print("2. Run: python phase5_upload_supabase.py")
print("\nAll CSVs are now aligned with expected schema!")
